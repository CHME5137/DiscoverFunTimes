{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Sensitivity Analysis to Interrogate Models\n",
    "\n",
    "Original notebook: Will Usher, UCL Energy Institute, 10th December 2015 <br/>\n",
    "Updates to demonstrate running array jobs on a cluster: Richard West, 2016\n",
    "\n",
    "In this version, most of the background and detail have been removed. Please refer to the original at https://github.com/SALib/SATut if you are not familiar with the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, interact\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(12, 10)\n",
    "sbn.set_context(\"talk\", font_scale=1)\n",
    "\n",
    "# The model used for this seminar is contained in the file model.py\n",
    "from model import cost_of_vehicle_to_grid, compute_profit, annualized_capital_cost, battery_lifetime, max_vehicle_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment and execute the following line to see the contents of the `model.py` file\n",
    "# %load model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using SALib to run a Sensitivity Analysis\n",
    "\n",
    "As we saw earlier, SALib is a **free** **open-source** **Python** library which you can install by running the command\n",
    "\n",
    "```python\n",
    "pip install SALib\n",
    "```\n",
    "\n",
    "[Documentation](http://salib.readthedocs.org/) is available online.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from SALib.sample import morris as ms\n",
    "from SALib.analyze import morris as ma\n",
    "from SALib.plotting import morris as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define a problem file\n",
    "\n",
    "In the code below, a problem file is used to define the variables we wish to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "morris_problem = {\n",
    "    # There are six variables\n",
    "    'num_vars': 7,\n",
    "    # These are their names\n",
    "    'names': ['conn', 'batt', 'dist', 'range', 'dri_eff', 'inv_eff', 'dispatch_time'],\n",
    "    # These are their plausible ranges over which we'll move the variables\n",
    "    'bounds': [[2.3, 22], # connection_power (kW)\n",
    "               [50, 100], # battery size (kWh)\n",
    "               [0, 80], # distance driven (km)\n",
    "               [0, 80], # range buffer (km)\n",
    "               [4,5.5], # driving efficiency (kWh/km)\n",
    "               [0.87,0.97], # inverter efficienct (%)\n",
    "               [0.5, 24] # dispatch time - hours of the day in which the energy is dispatched\n",
    "              ],\n",
    "    # I don't want to group any of these variables together\n",
    "    'groups': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generate a Sample\n",
    "\n",
    "We then generate a sample using the `morris.sample()` procedure from the SALib package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample array is  (8000, 7)\n",
      "Here are the first 10 rows:\n",
      "     8.867     83.333     53.333     26.667      5.500      0.903      8.333\n",
      "     8.867     83.333     53.333     26.667      5.500      0.970      8.333\n",
      "     8.867     83.333     53.333     26.667      4.500      0.970      8.333\n",
      "     8.867     83.333     53.333     80.000      4.500      0.970      8.333\n",
      "     8.867     83.333     53.333     80.000      4.500      0.970     24.000\n",
      "     8.867     50.000     53.333     80.000      4.500      0.970     24.000\n",
      "    22.000     50.000     53.333     80.000      4.500      0.970     24.000\n",
      "    22.000     50.000      0.000     80.000      4.500      0.970     24.000\n",
      "     8.867    100.000     26.667     80.000      5.500      0.937      8.333\n",
      "     8.867    100.000     26.667     80.000      5.500      0.870      8.333\n"
     ]
    }
   ],
   "source": [
    "number_of_trajectories = 1000\n",
    "sample = ms.sample(morris_problem, number_of_trajectories, num_levels=4, grid_jump=2)\n",
    "print(\"The sample array is \",sample.shape)\n",
    "print(\"Here are the first 10 rows:\")\n",
    "for j in range(10):\n",
    "    print(' '.join(['{:10.3f}'.format(i) for i in sample[j]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to save the parameters to a file, so we can run the jobs separately not in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"parameter_values.txt\", sample)\n",
    "\n",
    "# This creates a blank file to store results in:\n",
    "with open(\"results.txt\", 'w') as result_file:  # 'w' is write mode, and will clear the file.\n",
    "    result_file.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop!  \n",
    "## Now (pretend) we need to run the simulations on Discovery\n",
    "\n",
    "To run this on Discovery, you will need to copy the `parameter_values.txt` file on to Discovery, along with a Python script file (eg. `script.py`) that looks like the cell a few lines below.  Use an SCP or SFTP program, as described earlier in the tutorial.\n",
    "\n",
    "This is how many simulations we will need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but the Slurm on Discovery is configured with a maximum job array size of 1001. (Run `scontrol show config | grep MaxArraySize` on Discovery to check). So we will run 1000 jobs, each of which runs 8 simulations.  (In real life, split into as few jobs as reasonable, to avoid clogging Slurm with thousands of needless jobs. i.e. 100 jobs each of 80 simulations would be better, or 10 jobs of 800)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following cell contains the script.py. To update it uncomment the first `%load script.py` line and execute it - that will load in the `script.py` file which lives alongside this notebook and is where you should make changes if you are editing this tutorial.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load script.py\n",
    "# This is a script that you should run on Discovery,\n",
    "# as part of a Slurm Array job, with 1000 jobs.\n",
    "import numpy as np\n",
    "import os\n",
    "from model import max_vehicle_power\n",
    "big_parameter_list = np.loadtxt(\"parameter_values.txt\")\n",
    "job_number = int(os.getenv('SLURM_ARRAY_TASK_ID', default='0'))\n",
    "assert 0<=job_number<1000, \"Job number should run from 0 to 999\"\n",
    "for i in range(8):\n",
    "    parameter_number = (8 * job_number) + i\n",
    "    parameters = big_parameter_list[parameter_number]\n",
    "    result = max_vehicle_power(*parameters)\n",
    "    \"\"\"\n",
    "    Because we don't know what order the jobs will complete in,\n",
    "    the results may be written out of order!\n",
    "    To deal with this, we will write the job number in the results file\n",
    "    as well as the result:\n",
    "    \"\"\"\n",
    "    with open(\"results.txt\", 'a') as result_file: # 'a' is append mode, and will add to the file.\n",
    "        result_file.write('{} {}\\n'.format(parameter_number, result)) # the '\\n' is a new line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a `submit.sh` script to run it as an Array job, to fill the `results.txt` file with results.\n",
    "But realize that your Python script above expects the job number to start at zero, so you'll probably want something like\n",
    "```\n",
    "#SBATCH --array=0-999%40\n",
    "```\n",
    "in your submit file. eg. your `submit.sh` may look something like this:\n",
    "\n",
    "```\n",
    "#!/bin/sh\n",
    "#SBATCH -n 1\n",
    "#SBATCH -N 1\n",
    "#SBATCH --job-name=SA\n",
    "#SBATCH --array=0-999\n",
    "#SBATCH -p ser-par-10g\n",
    "python3 script.py\n",
    "```\n",
    "\n",
    "Once your jobs have all finished, copy the `results.txt` back to your computer and put it alongside this Notebook. Hopefully we can import it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.loadtxt(\"results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal world, that would have worked. But computers aren't ideal, and it probably didn't. Open the `results.txt` file in a text editor and take a look at it carefully. My results file starts out looking OK:\n",
    "\n",
    "```\n",
    "0 0.0\n",
    "1 0.0\n",
    "2 0.0\n",
    "3 0.0\n",
    "4 15.433333333333334\n",
    "5 0.0\n",
    "6 0.0\n",
    "```\n",
    "\n",
    "but there are some blank lines:\n",
    "\n",
    "```\n",
    "36 2.3\n",
    "37 2.3\n",
    "38 15.433333333333334\n",
    "\n",
    "113 3.2741205154998254\n",
    "39 15.433333333333334\n",
    "114 3.472446888816995\n",
    "```\n",
    "\n",
    "and occasionally really weird things:\n",
    "\n",
    "```\n",
    "75 0.0\n",
    "\n",
    "138 0.0\n",
    "139 0.0\n",
    "5227552275525\n",
    "78 5.979226458931255\n",
    "141 0.0\n",
    "658266513936\n",
    "```\n",
    "\n",
    "The problem is (probably) that the different compute nodes running different processes all trying to write to the same file, which is stored on a networked file system on yet another computer, are conflicting, instead of waiting their turn to write to the file. This is notoriously difficult to manage, as there are all sorts of levels of cacheing involved. \n",
    "\n",
    "What we'll try next is to split the 8000 jobs into 100 jobs each running 80 parameter sets, and have each job print the results into its standard output file, which Slurm will collect into 100 separate files. Then we'll concatenate the 100 output files when the jobs are all done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and analyzing the results \n",
    "Then come back here to load the results and continue the sensitivy analysis.\n",
    "Because our results file may not be in order, but contains the job number at the start of each line, we need to do a little manipulation to get the `output` array as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_array = np.loadtxt(\"results.txt\")\n",
    "results_dict = dict()\n",
    "for number, value in results_array:\n",
    "    results_dict[int(number)] = value\n",
    "results_dict\n",
    "output = np.array([results_dict[i] for i in range(len(results_dict))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Factor Prioritisation\n",
    "\n",
    "We'll run a sensitivity analysis to see which is the most influential parameter.\n",
    "\n",
    "The results parameters are called **mu**, **sigma** and **mu_star**.\n",
    "\n",
    "* **Mu** is the mean effect caused by the input parameter being moved over its range.\n",
    "* **Sigma** is the standard deviation of the mean effect.\n",
    "* **Mu_star** is the mean absolute effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Si = ma.analyze(morris_problem, sample, output, print_to_console=False)\n",
    "print(\"{:20s} {:>7s} {:>7s} {:>7s}\".format(\"Name\", \"mu\", \"mu_star\", \"sigma\"))\n",
    "for name, s1, st, mean in zip(morris_problem['names'], Si['mu'], Si['mu_star'], Si['sigma']):\n",
    "    print(\"{:20s} {:=7.2f} {:=7.2f} {:=7.2f}\".format(name, s1, st, mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "mp.horizontal_bar_plot(ax1, Si, param_dict={})\n",
    "mp.covariance_plot(ax2, Si, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
